# Prediction Assignment Writeup
### Mark Anderson
### Coursera Practical Machine Learning (predmachlearn-013)

## Abstract
Using data from an experiment to recognize the proper execution of weight lifting exercises, a model is build to predict
sensor data from the experiment.  Participants in the original study were asked to correctly perform an exercise, and to perform the exercise with 4 common errors.  From the data of the experiments, a random forest model was built to use the
collected data to predict whether the exercise was performed correctly or in one of the 4 error modes.  First a four models
were build to perform cross-validaion and verify that for the random forest process, Out Of Bag error rate matches 
observed cross-validation error.  Then a model was build with the full data set and use to predict the values of the
test set.  It was possible to construct an extremely accurate (perhaps overfit) model to correctly perdict all the values
of the test set.



## References
(Qualitative Activity Recognition of Weight Lifting Exercises)[http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf]

(Breiman, Out-Of-Bag Estimation)[http://www.stat.berkelry.edu/~breiman/OOBestimation.pdf]


## Goals
The goal is to correct perdict the class(e) of the exercise among 
(Class A) as specified, 
(Class B) the elbows to the front, 
(Class C) lifting the dumbbell, 
(Class D) lowering the dumbbell only halfway
(Class E) throwing the hips to the front.




## Data cleaning

The data was read in from the raw sources.   

```{r echo=TRUE}
train_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if (!file.exists("pml-training.csv")) {
  download.file(train_data_url, destfile='pml-training.csv', method='curl')
}
pml_train <- read.csv("pml-training.csv", na.strings = c("NA",""))
dim(pml_train)
```

```{r echo=TRUE}
testing_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists("pml-testing.csv")) {
  download.file(testing_data_url, destfile='pml-testing.csv', method='curl')
}
pml_test <- read.csv("pml-testing.csv", na.strings = c("NA",""))
dim(pml_test)
```

### Tidy up data

Examining the date in a text editor
it was apparent that there were two
classes of data present, those with aggregate values and those without.  
The vast majority of the data was simply raw data.  The choice was made
to set aside the data on rows where the value of the column new_window
was yes.  In addition, the first 7 columns were experimental artifacts
like IDs and timestamps and thus were also selected for removal.

```{r echo=TRUE}
tidy_train <- pml_train[!grepl("yes",pml_train$new_window),]
artifacts <- c(1:7)
cn_kurtosis  <- grep("^kurtosis_", colnames(tidy_train))
cn_skewness  <- grep("^skewness_", colnames(tidy_train))
cn_max       <- grep("^max_",      colnames(tidy_train))
cn_min       <- grep("^min_",      colnames(tidy_train))
cn_var       <- grep("^var_",      colnames(tidy_train))
cn_avg       <- grep("^avg_",      colnames(tidy_train))
cn_stddev    <- grep("^stddev_",   colnames(tidy_train))
cn_total     <- grep("^total_",      colnames(tidy_train))
cn_amplitude <- grep("^amplitude_",colnames(tidy_train))

exclusions <- c(artifacts, cn_kurtosis, cn_skewness, cn_max, cn_min,
               cn_var, cn_avg, cn_stddev, cn_total,  cn_amplitude)
tidy_train = tidy_train[,-exclusions]
dim(tidy_train)
colnames(tidy_train)
```


## Cross Validation

The model building method choosen to for this assignment was Random Forest.
For Random Forest Out-of-bag estimates can used, instead of cross validation,
to predict out of sample error. (Breiman, Out-Of-Bag Estimation)[http://www.stat.berkelry.edu/~breiman/OOBestimation.pdf].
None the less, 4-fold cross validation was initialy done to observe this effect.
After observing the match of cross validation and out-of-bag error estimates,
A Random Forest model was constructed with all the training data.

## Out of Sample Error

```{r echo=TRUE}
library(doMC)
registerDoMC(cores=2)

Sys.time()
library(caret)
library(randomForest)
set.seed(1018)
folds <- createFolds(y=tidy_train$classe,k=4,returnTrain=FALSE)

train_1 <- tidy_train[-folds[[1]],]
test_1  <- tidy_train[ folds[[1]],]
train_2 <- tidy_train[-folds[[2]],]
test_2  <- tidy_train[ folds[[2]],]
train_3 <- tidy_train[-folds[[3]],]
test_3  <- tidy_train[ folds[[3]],]
train_4 <- tidy_train[-folds[[4]],]
test_4  <- tidy_train[ folds[[4]],]

model_rf_1 <- randomForest(classe ~ ., train_1, ntree=200, norm.votes=FALSE)
model_rf_1
prediction_rf_1 <- predict(model_rf_1,test_1)
confusionMatrix(prediction_rf_1,test_1$classe)$overall[1]

model_rf_2 <- randomForest(classe ~ ., train_2, ntree=200, norm.votes=FALSE)
model_rf_2
prediction_rf_2 <- predict(model_rf_2,test_2)
confusionMatrix(prediction_rf_2,test_2$classe)$overall[1]

model_rf_3 <- randomForest(classe ~ ., train_3, ntree=200, norm.votes=FALSE)
model_rf_3
prediction_rf_3 <- predict(model_rf_3,test_3)
confusionMatrix(prediction_rf_3,test_3$classe)$overall[1]


model_rf_4 <- randomForest(classe ~ ., train_4, ntree=200, norm.votes=FALSE)
model_rf_4
prediction_rf_4 <- predict(model_rf_4,test_4)
confusionMatrix(prediction_rf_4,test_4$classe)$overall[1]

model_rf <- randomForest(classe ~ ., tidy_train, ntree=200, norm.votes=FALSE)
model_rf
```

## Prediction and submission

The final model, build with all the training data, 'model_rf', was run
against the set aside training data.

```{r echo=TRUE}
answers <- predict(model_rf, pml_test)
```


The function provided to the class was used to write files
for submission to the grading process.
```{r echo=TRUE}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```

## Results
The model was able predict all 20 of the test cases correctly.

